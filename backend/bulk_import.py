"""
1íšŒì„± bulk import ìŠ¤í¬ë¦½íŠ¸
page2ì˜ ê³¼ê±° ê¸°ì‚¬ë“¤ì„ DBì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©

ì£¼ì˜: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìˆ˜ë™ìœ¼ë¡œ 1íšŒë§Œ ì‹¤í–‰í•´ì•¼ í•¨.
Deployí•  ë•Œë§ˆë‹¤ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜.
"""

from scraper import get_articles_from_page
from db import save_new_links
import sys

def import_page2_articles():
    """
    Page 2ì˜ ê¸°ì‚¬ë“¤ì„ DBì— bulk import
    """
    print("Page 2 ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
    articles = get_articles_from_page(2)

    if not articles:
        print("Page 2ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"Page 2ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    print("ìƒ˜í”Œ ê¸°ì‚¬ë“¤:")
    for i, (url, title, published_at) in enumerate(articles[:3]):
        if published_at:
            print(f"  {i+1}. [{published_at}] {title[:40]}...")
        else:
            print(f"  {i+1}. {title[:50]}...")

    print("\nDBì— ì €ì¥ ì‹œì‘...")
    new_articles = save_new_links(articles)

    print(f"\nâœ… ì™„ë£Œ! {len(new_articles)}ê°œ ìƒˆë¡œìš´ ê¸°ì‚¬ ì¶”ê°€ë¨")
    if len(new_articles) < len(articles):
        print(f"   (ì¤‘ë³µëœ {len(articles) - len(new_articles)}ê°œëŠ” ê±´ë„ˆëœ€)")

    return len(new_articles)

if __name__ == "__main__":
    print("=" * 50)
    print("ë‹¤ë‹ˆì—˜ê¸°ë„íšŒ ë‰´ìŠ¤ ê³¼ê±° ê¸°ì‚¬ Bulk Import")
    print("=" * 50)

    try:
        new_count = import_page2_articles()
        if new_count > 0:
            print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ {new_count}ê°œ ê³¼ê±° ê¸°ì‚¬ ì¶”ê°€ë¨!")
        else:
            print("\nâš ï¸  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜ ëª¨ë‘ ì¤‘ë³µë¨")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
